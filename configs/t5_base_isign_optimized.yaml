# Optimized Configuration for T5-Base on iSign Dataset
# Target: <20 min/epoch (ideally <15 min) on 3× A40 GPUs
# Launch: CUDA_VISIBLE_DEVICES=0,1,3 bash scripts/train_multi_gpu_optimized.sh configs/t5_base_isign_optimized.yaml 3

# Data Configuration
data:
  train_path: "/data/dept_share/sanjeet/dattatreya/jan8A40/rtmfeaturesisign/translationexperiment/tokenization/train_split_unicode_filtered_matched.csv"
  val_path: "/data/dept_share/sanjeet/dattatreya/jan8A40/rtmfeaturesisign/translationexperiment/tokenization/val_split_unicode_filtered_matched.csv"
  test_path: "/data/dept_share/sanjeet/dattatreya/jan8A40/rtmfeaturesisign/translationexperiment/tokenization/test_split_unicode_filtered_matched.csv"
  pose_dir: "/data/dept_share/sanjeet/dattatreya/jan8A40/rtmfeaturesisign/performance"
  
  max_frames: 300
  max_length: 128
  step_frames: 5
  num_keypoints: 266

# Model Configuration
model:
  name: "t5-base"
  tokenizer: "t5-base"
  
  dropout: 0.1
  freeze_encoder: false
  freeze_decoder: false
  
  params: {}
  
  special_tokens:
    additional_special_tokens: ["<PERSON>", "<UNKNOWN>"]

# Training Configuration
training:
  num_epochs: 100
  batch_size: 8
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  
  max_grad_norm: 0.5
  gradient_accumulation_steps: 4
  
  # Mixed precision with bf16 (A40 supports bf16 natively)
  mixed_precision: true
  mixed_precision_dtype: "bf16"
  
  warmup_ratio: 0.05
  
  checkpoint_dir: "checkpoints/t5_base_isign_optimized"
  save_every: 5
  eval_every: 1
  
  num_beams: 5
  max_gen_length: 128
  
  use_wandb: true
  project_name: "sign-language-translation"
  run_name: "t5_base_isign_optimized"
  
  # MUST be 0 — any value >0 causes DDP hangs on shared filesystem
  num_workers: 0

# Performance Optimizations
optimizations:
  # Pre-load all pickle pose files into memory at startup
  # Eliminates per-batch NFS I/O bottleneck (biggest speedup)
  cache_in_memory: true
  
  # Enable cudnn.benchmark for faster CUDA operations
  cudnn_benchmark: true
  
  # DDP static_graph optimization — reduces communication overhead
  # Safe because the computation graph is the same every iteration
  static_graph: true
  
  # DDP gradient bucket size in MB (default 25)
  ddp_bucket_cap_mb: 25

  # torch.compile JIT compilation (PyTorch 2.0+)
  # Set to true for additional speedup; adds warmup time on first iteration
  torch_compile: false
  compile_mode: "default"

seed: 42
