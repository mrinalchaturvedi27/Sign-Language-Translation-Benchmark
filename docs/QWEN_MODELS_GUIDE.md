# Qwen Models for Sign Language Translation üöÄ

**Complete guide to choosing the right Qwen model for your task**

---

## üìä Qwen Model Lineup (2024)

### Qwen 2.5 Family (October 2024) ‚≠ê **RECOMMENDED**

| Model | Size | Context | Best For | Config |
|-------|------|---------|----------|--------|
| Qwen2.5-0.5B | 0.5B | 32K | Edge/Mobile | - |
| Qwen2.5-1.5B | 1.5B | 32K | Fast experiments | - |
| Qwen2.5-3B | 3B | 32K | Baseline testing | - |
| **Qwen2.5-7B** | 7B | 128K | **Recommended** | `qwen2.5_7b_isign.yaml` |
| **Qwen2.5-7B-Instruct** | 7B | 128K | **Best overall** | `qwen2.5_7b_instruct_isign.yaml` |
| **Qwen2.5-14B** | 14B | 128K | **Best quality** | `qwen2.5_14b_isign.yaml` |
| Qwen2.5-32B | 32B | 128K | SOTA | - |
| Qwen2.5-72B | 72B | 128K | Research only | - |

### Qwen 2 Family (April 2024)

| Model | Size | Notes |
|-------|------|-------|
| Qwen2-7B | 7B | Older version, use Qwen2.5 instead |
| Qwen2-72B | 72B | Older version, use Qwen2.5 instead |

### QwQ-32B (December 2024) - "Qwen 3"

| Model | Size | Best For |
|-------|------|----------|
| QwQ-32B-Preview | 32B | ‚ùå **NOT for translation** - designed for math/reasoning |

---

## üéØ Which Qwen Model Should I Use?

### Decision Tree

```
What's your goal?
‚îÇ
‚îú‚îÄ Quick experiments / Limited GPU (<12GB)
‚îÇ  ‚îî‚îÄ Use: Qwen2.5-7B-Instruct (4-bit) ‚≠ê
‚îÇ
‚îú‚îÄ Best quality on standard GPU (24GB)
‚îÇ  ‚îî‚îÄ Use: Qwen2.5-14B (4-bit) ‚≠ê‚≠ê
‚îÇ
‚îú‚îÄ Maximum quality / Research paper
‚îÇ  ‚îî‚îÄ Use: Qwen2.5-32B (4-bit, multi-GPU) ‚≠ê‚≠ê‚≠ê
‚îÇ
‚îî‚îÄ Mathematical reasoning (NOT translation)
   ‚îî‚îÄ Use: QwQ-32B (different use case)
```

### Recommended Choices

#### ü•á Best Overall: **Qwen2.5-7B-Instruct**
```bash
bash train_multi_gpu.sh configs/qwen2.5_7b_instruct_isign.yaml 4
```

**Why?**
- ‚úÖ Instruction-tuned (follows prompts better)
- ‚úÖ Excellent multilingual support
- ‚úÖ Fits in 10GB GPU with 4-bit
- ‚úÖ Great balance of speed and quality
- ‚úÖ Active development and support

**Expected Performance:**
- BLEU-4: **7-9**
- ROUGE-L: **0.42-0.47**
- Training Time: **18-20h** (4x A100)

#### ü•à Best Quality: **Qwen2.5-14B**
```bash
bash train_multi_gpu.sh configs/qwen2.5_14b_isign.yaml 4
```

**Why?**
- ‚úÖ Larger capacity
- ‚úÖ Better understanding of complex patterns
- ‚úÖ Still fits in 16GB GPU with 4-bit
- ‚úÖ Worth the extra compute

**Expected Performance:**
- BLEU-4: **8-10**
- ROUGE-L: **0.45-0.50**
- Training Time: **24-28h** (4x A100)

#### ü•â Budget Option: **Qwen2.5-7B** (Base)
```bash
bash train_multi_gpu.sh configs/qwen2.5_7b_isign.yaml 4
```

**Why?**
- ‚úÖ No instruction tuning overhead
- ‚úÖ Slightly faster
- ‚úÖ Same memory footprint as Instruct

**Expected Performance:**
- BLEU-4: **6-8**
- ROUGE-L: **0.40-0.45**
- Training Time: **16-18h** (4x A100)

---

## üìä Detailed Comparison

### Qwen 2.5-7B vs Qwen 2.5-7B-Instruct

| Feature | Base | Instruct |
|---------|------|----------|
| Pre-training | General text | General text |
| Post-training | None | Instruction tuning |
| Translation Quality | Good | **Better** |
| Instruction Following | Moderate | **Excellent** |
| Recommended For | Research baselines | **Production** |
| Expected BLEU-4 | 6-8 | **7-9** |

**Verdict:** Use **Instruct** unless you specifically need base model for research comparison.

### Qwen 2.5-7B-Instruct vs Qwen 2.5-14B

| Feature | 7B-Instruct | 14B |
|---------|-------------|-----|
| Parameters | 7B | 14B |
| Memory (4-bit) | 10GB | 16GB |
| Training Speed | Faster | Slower |
| Quality | Excellent | **Better** |
| Recommended For | **Most users** | Best quality |
| Expected BLEU-4 | 7-9 | **8-10** |

**Verdict:** Use **7B-Instruct** for most cases, **14B** if you need absolute best quality and have GPU memory.

---

## üÜö Qwen vs Competitors

### Sign Language Translation Performance (Estimated)

| Model | BLEU-4 | Speed | Memory (4-bit) |
|-------|--------|-------|----------------|
| T5-Base | 3-5 | ‚ö°‚ö°‚ö° | 12GB |
| T5-Large | 4-6 | ‚ö°‚ö° | 24GB |
| **Qwen2.5-7B-Instruct** | **7-9** | ‚ö°‚ö° | **10GB** |
| **Qwen2.5-14B** | **8-10** | ‚ö° | **16GB** |
| Gemma-7B | 5-7 | ‚ö°‚ö° | 10GB |
| Llama-3.1-8B | 6-8 | ‚ö°‚ö° | 12GB |
| Mistral-7B | 6-8 | ‚ö°‚ö° | 10GB |

### Why Qwen 2.5 Often Wins

1. **Better Multilingual:** Trained on more diverse languages
2. **Longer Context:** 128K vs 8K for Llama
3. **Recent Training:** October 2024 (fresher data)
4. **Instruction Tuning:** Instruct version follows prompts excellently
5. **Open Weights:** No restrictions (unlike Llama)

---

## üíæ Memory Requirements

### Qwen 2.5-7B-Instruct

| Configuration | Memory | GPUs |
|---------------|--------|------|
| FP16 (no LoRA) | 28GB | 1x A100 |
| FP16 + LoRA | 16GB | 1x RTX 4090 |
| 8-bit + LoRA | 12GB | 1x RTX 3090 |
| **4-bit + LoRA** | **10GB** | **1x RTX 3080** ‚≠ê |

### Qwen 2.5-14B

| Configuration | Memory | GPUs |
|---------------|--------|------|
| FP16 (no LoRA) | 56GB | 2x A100 |
| FP16 + LoRA | 32GB | 1x A100 40GB |
| 8-bit + LoRA | 20GB | 1x RTX 4090 |
| **4-bit + LoRA** | **16GB** | **1x RTX 4090** ‚≠ê |

---

## ‚öôÔ∏è Configuration Examples

### Standard Configuration (Qwen2.5-7B-Instruct)

```yaml
model:
  name: "Qwen/Qwen2.5-7B-Instruct"
  tokenizer: "Qwen/Qwen2.5-7B-Instruct"
  
  use_lora: true
  load_in_4bit: true  # Recommended
  
  lora_config:
    r: 16
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    lora_dropout: 0.1

training:
  batch_size: 4
  learning_rate: 1e-4  # Lower for instruct models
  gradient_accumulation_steps: 8
```

### High-Quality Configuration (Qwen2.5-14B)

```yaml
model:
  name: "Qwen/Qwen2.5-14B"
  
  use_lora: true
  load_in_4bit: true
  
  lora_config:
    r: 32  # Higher rank
    lora_alpha: 64
    # Target ALL modules for best quality
    target_modules: [
      "q_proj", "k_proj", "v_proj", "o_proj",
      "gate_proj", "up_proj", "down_proj"
    ]

training:
  batch_size: 2
  learning_rate: 1e-4
  gradient_accumulation_steps: 16
```

---

## üöÄ Quick Start

### Method 1: Use Pre-made Configs

```bash
# Best overall (7B-Instruct)
bash train_multi_gpu.sh configs/qwen2.5_7b_instruct_isign.yaml 4

# Best quality (14B)
bash train_multi_gpu.sh configs/qwen2.5_14b_isign.yaml 4

# Base model (research)
bash train_multi_gpu.sh configs/qwen2.5_7b_isign.yaml 4
```

### Method 2: Create Custom Config

```yaml
# configs/qwen2.5_custom.yaml
model:
  name: "Qwen/Qwen2.5-7B-Instruct"
  tokenizer: "Qwen/Qwen2.5-7B-Instruct"
  use_lora: true
  load_in_4bit: true
  # ... rest of config
```

---

## üéì Best Practices

### LoRA Settings for Qwen

| Model Size | r | lora_alpha | target_modules |
|------------|---|------------|----------------|
| 7B | 16 | 32 | q,k,v,o |
| 14B | 32 | 64 | q,k,v,o,gate,up,down |
| 32B+ | 64 | 128 | All modules |

### Training Hyperparameters

| Model | Learning Rate | Batch Size | Grad Accum |
|-------|--------------|------------|------------|
| 7B (base) | 2e-4 | 4 | 8 |
| 7B-Instruct | **1e-4** | 4 | 8 |
| 14B | **1e-4** | 2 | 16 |
| 32B | **5e-5** | 1 | 32 |

**Note:** Instruct models need lower learning rates than base models!

---

## ‚ùå Common Mistakes

### ‚ùå Using QwQ for Translation

```yaml
# DON'T DO THIS
model:
  name: "Qwen/QwQ-32B-Preview"  # This is for MATH, not translation!
```

**Why:** QwQ is optimized for reasoning, not translation. Use Qwen2.5 instead.

### ‚ùå Using Old Qwen2 Instead of Qwen2.5

```yaml
# DON'T DO THIS
model:
  name: "Qwen/Qwen2-7B"  # Old version

# DO THIS INSTEAD
model:
  name: "Qwen/Qwen2.5-7B"  # New version, better performance
```

### ‚ùå Not Using 4-bit Quantization

```yaml
# INEFFICIENT
model:
  load_in_4bit: false  # Uses 28GB for 7B model!

# EFFICIENT
model:
  load_in_4bit: true  # Uses only 10GB for 7B model!
```

---

## üìà Expected Results

Based on iSign dataset (118k ISL-English samples):

### After 30 Epochs

| Model | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L |
|-------|--------|--------|--------|--------|---------|
| Qwen2.5-7B | 45-50 | 30-35 | 18-22 | 6-8 | 0.40-0.45 |
| **Qwen2.5-7B-Instruct** | **48-53** | **33-38** | **21-25** | **7-9** | **0.42-0.47** |
| **Qwen2.5-14B** | **52-57** | **37-42** | **24-28** | **8-10** | **0.45-0.50** |

---

## üîß Troubleshooting

### Model Download is Slow

```bash
# Use a mirror or cache
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME="/path/to/large/storage"
```

### OOM Even with 4-bit

```bash
# Reduce batch size
training:
  batch_size: 2  # Instead of 4
  gradient_accumulation_steps: 16  # Instead of 8
```

### Instruct Model Not Following Format

```python
# Make sure tokenizer is set correctly
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
# Qwen Instruct models use ChatML format internally
```

---

## üìö Resources

- **Qwen2.5 Release:** https://qwenlm.github.io/blog/qwen2.5/
- **Qwen GitHub:** https://github.com/QwenLM/Qwen2.5
- **QwQ Release (reasoning):** https://qwenlm.github.io/blog/qwq-32b-preview/
- **Model Cards:** https://huggingface.co/Qwen

---

## ‚úÖ Final Recommendations

### For Most Users
```bash
bash train_multi_gpu.sh configs/qwen2.5_7b_instruct_isign.yaml 4
```
- Best balance of quality, speed, and memory
- Instruction-tuned for better performance
- Fits in 10GB GPU with 4-bit

### For Best Quality
```bash
bash train_multi_gpu.sh configs/qwen2.5_14b_isign.yaml 4
```
- Higher capacity
- Better complex pattern recognition
- Worth the extra compute time

### For Research Baseline
```bash
bash train_multi_gpu.sh configs/qwen2.5_7b_isign.yaml 4
```
- Base model without instruction tuning
- Good for controlled experiments

---

**Summary:** Qwen 2.5-7B-Instruct is the **sweet spot** for sign language translation! üéØ
