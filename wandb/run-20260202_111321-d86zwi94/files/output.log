2026-02-02 11:13:22,701 - src.trainers.trainer - INFO - Trainer initialized on rank 0/1
2026-02-02 11:13:22,702 - src.trainers.trainer - INFO - Starting training...
Epoch 1:   0%|                                                                                                | 0/11258 [00:00<?, ?it/s]/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/src/trainers/trainer.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=self.mixed_precision):
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Epoch 1:   0%|                                                            | 6/11258 [00:18<9:42:27,  3.11s/it, loss=4.1931, lr=2.37e-08]
Traceback (most recent call last):
  File "/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/train.py", line 176, in <module>
    main(args.config)
    ~~~~^^^^^^^^^^^^^
  File "/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/train.py", line 156, in main
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/src/trainers/trainer.py", line 276, in train
    train_loss = self.train_epoch(epoch)
  File "/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/src/trainers/trainer.py", line 127, in train_epoch
    outputs = self.model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels
    )
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1881, in _call_impl
    return inner()
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1829, in inner
    result = forward_call(*args, **kwargs)
  File "/data/dept_share/sanjeet/ugp-26/Mrinal/Sign-Language-Translation-Benchmark/src/models/model_factory.py", line 131, in forward
    outputs = self.model(
        inputs_embeds=combined_embeds,
    ...<2 lines>...
        return_dict=True
    )
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/peft/peft_model.py", line 1923, in forward
    return self.base_model(
           ~~~~~~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/peft/tuners/tuners_utils.py", line 311, in forward
    return self.model.forward(*args, **kwargs)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 449, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 384, in forward
    hidden_states = decoder_layer(
        hidden_states,
    ...<6 lines>...
        **kwargs,
    )
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/modeling_layers.py", line 93, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
    return fn(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/utils/checkpoint.py", line 496, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/autograd/function.py", line 581, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 234, in forward
    hidden_states, _ = self.self_attn(
                       ~~~~~~~~~~~~~~^
        hidden_states=hidden_states,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 155, in forward
    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                   ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/peft/tuners/lora/bnb.py", line 569, in forward
    output = lora_B(lora_A(dropout(x))) * scaling
             ~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ashishu23/miniconda3/lib/python3.13/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
